{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CX4240 project\n",
    "\n",
    "## Classification of Acute Lymphoblastic Leukemia (ALL) in Blood Cell Images Using Machine Learning\n",
    "\n",
    "# Classifications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stat\n",
    "import sklearn.preprocessing as pre\n",
    "import glob\n",
    "import mahotas as mt\n",
    "import pywt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import csv \n",
    "import matplotlib as mpl\n",
    "import time\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from mlens.ensemble import SuperLearner\n",
    "from mlens.metrics import make_scorer\n",
    "\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('Data/PCA_train_data.csv')\n",
    "train = pd.DataFrame.as_matrix(train_df)\n",
    "test = pd.DataFrame.as_matrix(pd.read_csv('Data/PCA_test_data.csv'))\n",
    "train_data = train[:,2:]\n",
    "label_train = np.array(train[:,1], dtype=int)\n",
    "test_data = test[:,2:]\n",
    "label_test = np.array(test[:,1], dtype=int)\n",
    "features = list(train_df.columns[2:]) \n",
    "print(np.shape(train_data))\n",
    "print(np.shape(test_data))\n",
    "n_ALL = 7272\n",
    "n_hem = 3389\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_errors(prediction, label):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        label: numpy array of ground truth (1 is positive, 0 is negative)\n",
    "        prediction: numpy array of prediction\n",
    "    returns:\n",
    "        false_positive: integer of occurences of false positives\n",
    "        false_negative: integer of occurences of false negatives\n",
    "        true_positive: integer of occurences of true positives\n",
    "        true_negative: integer of occurences of true negatives\n",
    "    \"\"\"\n",
    "    diff = label - prediction\n",
    "    false_positive = np.count_nonzero(diff == -1)\n",
    "    false_negative = np.count_nonzero(diff == 1)\n",
    "    total_positive = np.sum(label)\n",
    "    total_negative = int(len(label) - total_positive)\n",
    "    true_positive = int(total_positive - false_negative)\n",
    "    true_negative = total_negative - false_positive\n",
    "    accuracy = np.round((true_positive + true_negative) / \n",
    "                                       (total_positive + total_negative), decimals=3)\n",
    "    recall =  np.round(true_positive / (true_positive + false_positive), decimals=3)\n",
    "    precision = np.round(true_positive / (true_positive + false_negative), decimals=3)\n",
    "    F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    print('True Positive:  ',true_positive)\n",
    "    print('False Positive: ',false_positive)\n",
    "    print('True Negative:  ',true_negative)\n",
    "    print('False Negative: ',false_negative)\n",
    "    print('Precision:      ', precision)\n",
    "    print('Recall:         ',recall )\n",
    "    print('Accuracy:       ',accuracy)\n",
    "    return accuracy, recall, precision, F1\n",
    "\n",
    "\n",
    "def plot_grid_search(cv_results, grid_param_1, grid_param_2, name_param_1, name_param_2):\n",
    "    # Get Test Scores Mean and std for each grid search\n",
    "    scores_mean = cv_results['mean_test_score']\n",
    "    scores_mean = np.array(scores_mean).reshape(len(grid_param_2),len(grid_param_1))\n",
    "\n",
    "    scores_sd = cv_results['std_test_score']\n",
    "    scores_sd = np.array(scores_sd).reshape(len(grid_param_2),len(grid_param_1))\n",
    "\n",
    "    # Plot Grid search scores\n",
    "    _, ax = plt.subplots(1,1)\n",
    "\n",
    "    # Param1 is the X-axis, Param 2 is represented as a different curve (color line)\n",
    "    for idx, val in enumerate(grid_param_2):\n",
    "        ax.plot(grid_param_1, scores_mean[idx,:], '-o', label= name_param_2 + ': ' + str(val))\n",
    "\n",
    "    ax.set_title(\"Grid Search Scores\", fontsize=20, fontweight='bold')\n",
    "    ax.set_xlabel(name_param_1, fontsize=16)\n",
    "    ax.set_ylabel('CV Average Score', fontsize=16)\n",
    "    ax.legend(loc=\"best\", fontsize=15)\n",
    "    ax.grid('on')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "# Tuning the hyperparameters\n",
    "param_grid =  {'C': [1,10,25], 'gamma': [.1,1,3,6,10], 'kernel': ['rbf'], 'probability':[True]}\n",
    "SV_clf = GridSearchCV(svm.SVC(), param_grid, cv=5)\n",
    "t0 = time.time()\n",
    "SV_clf.fit(train_data, label_train)\n",
    "svc_fit = time.time() - t0\n",
    "print(\"SVM gamma and C selected and model fitted in %.2f s\"\n",
    "      % svc_fit)\n",
    "\n",
    "\n",
    "# save the best hyperparameters\n",
    "svm_c, svm_gamma = SV_clf.best_estimator_.C, SV_clf.best_estimator_.gamma\n",
    "\n",
    "print('best score:', SV_clf.best_score_)                               \n",
    "print('best C value:', svm_c)\n",
    "print('best gamma value:', svm_gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = [1,10,25]\n",
    "Gammas = [.1,1,3,6,10]\n",
    "    \n",
    "# visualizing gridsearch\n",
    "plot_grid_search(SV_clf.cv_results_, Gammas, Cs, 'gamma', 'C')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how it works on the training data\n",
    "print('SVM Results')\n",
    "print('-------------------------')\n",
    "print('Training Data:')\n",
    "train_predictions = SV_clf.predict(train_data)\n",
    "get_errors(train_predictions, label_train)\n",
    "\n",
    "\n",
    "# see how it works on the test data\n",
    "print(' ')\n",
    "print('Test Data:')\n",
    "test_predictions = SV_clf.predict(test_data)\n",
    "SVM_accuracy,SVM_recall,SVM_precision,SVM_F1 = get_errors(test_predictions, label_test)\n",
    "\n",
    "\n",
    "accuracies = [SVM_accuracy]\n",
    "recalls = [SVM_recall]\n",
    "precisions = [SVM_precision]\n",
    "F1 = [SVM_F1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning the hyperparameters\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "RF_clf = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 50, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "RF_clf.fit(train_data, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print optomized parameters\n",
    "print(RF_clf.best_estimator_)\n",
    "\n",
    "\n",
    "# see how it works on the training data\n",
    "print('RF Results')\n",
    "print('-------------------------')\n",
    "print('Training Data:')\n",
    "train_predictions = RF_clf.predict(train_data)\n",
    "get_errors(train_predictions, label_train)\n",
    "\n",
    "\n",
    "# see how it works on the test data\n",
    "print(' ')\n",
    "print('Test Data:')\n",
    "test_predictions = RF_clf.predict(test_data)\n",
    "RF_accuracy, RF_recall, RF_precision, RF_F1 = get_errors(test_predictions, label_test)\n",
    "\n",
    "accuracies.append(RF_accuracy)\n",
    "recalls.append(RF_recall)\n",
    "precisions.append(RF_precision)\n",
    "F1.append(RF_F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more conservative rf\n",
    "rf2 = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=75, max_features='sqrt', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=3, min_samples_split=5,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=None,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "\n",
    "# fit\n",
    "rf2.fit(train_data, label_train)\n",
    "\n",
    "\n",
    "# see how it works on the training data\n",
    "print('RF Results')\n",
    "print('-------------------------')\n",
    "print('Training Data:')\n",
    "train_predictions = rf2.predict(train_data)\n",
    "get_errors(train_predictions, label_train)\n",
    "\n",
    "\n",
    "# see how it works on the test data\n",
    "print(' ')\n",
    "print('Test Data:')\n",
    "test_predictions = rf2.predict(test_data)\n",
    "RF2_accuracy,RF2_recall,RF2_precision,RF2_F1 = get_errors(test_predictions, label_test)\n",
    "\n",
    "accuracies.append(RF2_accuracy)\n",
    "recalls.append(RF2_recall)\n",
    "precisions.append(RF2_precision)\n",
    "F1.append(RF2_F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find best K values using cross validation\n",
    "def k_fold_test(train_data, train_label, train_index, test_index, k_value):\n",
    "    KNN = KNeighborsClassifier(n_neighbors = k_value, weights=\"uniform\", algorithm=\"brute\") \n",
    "    KNN.fit(train_data[train_index,:], train_label[train_index])\n",
    "    mean_accuracy_current_fold = KNN.score(train_data[test_index,:], train_label[test_index])\n",
    "    return mean_accuracy_current_fold\n",
    "\n",
    "def k_value_use_k_fold(train_data, train_label, k_value, kfold_nsplit):\n",
    "    all_mean_accuracy = []\n",
    "    kf = KFold(n_splits = kfold_nsplit, shuffle = False)\n",
    "    for train_index, test_index in kf.split(train_data):\n",
    "        mean_accuracy_current_fold = k_fold_test(train_data, train_label, train_index, test_index, k_value)\n",
    "        all_mean_accuracy.append(mean_accuracy_current_fold)\n",
    "    average_accuracy = np.round(np.mean(np.array(all_mean_accuracy)), decimals=4)\n",
    "    print(\"{} \\t {}\".format(k_value, average_accuracy))\n",
    "    return average_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the mean accuracy for each k value\n",
    "\n",
    "number_of_splits = 10\n",
    "k_value_list = [4,6,8,9,10,12,15,20,25,30, 50, 90, 100, 150, 200]\n",
    "\n",
    "print('K val \\t Avg. Accuracy')\n",
    "print('--------------------------')\n",
    "test_tuple_list = []\n",
    "for k_value in k_value_list:\n",
    "    average_accuracy = k_value_use_k_fold(train_data, label_train, k_value, number_of_splits)\n",
    "    test_tuple_list.append((average_accuracy, k_value))\n",
    "    \n",
    "x = [x[1] for x in test_tuple_list]\n",
    "y = [x[0] for x in test_tuple_list]\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(x,y)\n",
    "plt.plot(x,y,'or')\n",
    "plt.title(\"K Value vs. Average Accuracy\", fontsize=15)\n",
    "plt.xlabel(\"K Value\", fontsize=10)\n",
    "plt.ylabel(\"Average Accuracy\", fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "test_tuple_list.sort(reverse = True)\n",
    "best_k_value = test_tuple_list[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best K_value\n",
    "\n",
    "print(\"Best K Value is {}\".format(best_k_value))\n",
    "print(' ')\n",
    "KNN_clf = KNeighborsClassifier(n_neighbors = best_k_value, weights=\"uniform\", algorithm=\"brute\")\n",
    "\n",
    "\n",
    "# train and predict\n",
    "KNN_clf.fit(train_data, label_train)\n",
    "KNN_train_pred = KNN_clf.predict(train_data)\n",
    "KNN_test_pred = KNN_clf.predict(test_data)\n",
    "\n",
    "\n",
    "# print results\n",
    "# train results\n",
    "print('KNN Results')\n",
    "print('-------------------------')\n",
    "print('Training Data')\n",
    "get_errors(KNN_train_pred, label_train)\n",
    "\n",
    "# test results\n",
    "print(' ')\n",
    "print('Test Data')\n",
    "KNN_accuracy,KNN_recall,KNN_precision,KNN_F1 = get_errors(KNN_test_pred, label_test)\n",
    "\n",
    "accuracies.append(KNN_accuracy)\n",
    "recalls.append(KNN_recall)\n",
    "precisions.append(KNN_precision)\n",
    "F1.append(KNN_F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "#import xgboost as xgb\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "seed = 2034\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we combine the three models above (SVM, RF and KNN)\n",
    "# and make a prediction with majority vote\n",
    "\n",
    "# first create models with the optomizal hyperparameters discerned above\n",
    "svc = svm.SVC(C=svm_c, gamma=svm_gamma, kernel='rbf', probability=True)\n",
    "rf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=110, max_features='sqrt', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=None,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "knn = KNeighborsClassifier(n_neighbors = best_k_value, weights=\"uniform\", algorithm=\"brute\")\n",
    "\n",
    "# store the estimators in a list for our voting classifier\n",
    "ests = [('svc', svc), ('rf', rf), ('knn', knn)]\n",
    "\n",
    "# create the max-vote classifier\n",
    "MV_clf = VotingClassifier(estimators=ests, voting='soft')\n",
    "\n",
    "# train and predict\n",
    "MV_clf.fit(train_data, label_train)\n",
    "mv_train_pred = MV_clf.predict(train_data)\n",
    "mv_test_pred = MV_clf.predict(test_data)\n",
    "\n",
    "\n",
    "# print results\n",
    "# train results\n",
    "print('Max Voting Results')\n",
    "print('-------------------------')\n",
    "print('Training Data')\n",
    "get_errors(mv_train_pred, label_train)\n",
    "\n",
    "# test results\n",
    "print(' ')\n",
    "print('Test Data')\n",
    "MV_accuracy,MV_recall,MV_precision,MV_F1 = get_errors(mv_test_pred, label_test)\n",
    "\n",
    "accuracies.append(MV_accuracy)\n",
    "recalls.append(MV_recall)\n",
    "precisions.append(MV_precision)\n",
    "F1.append(MV_F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we use SVM, RF and KNN as our base estimators\n",
    "n_folds = 5\n",
    "N = train_data.shape[0]\n",
    "n_ests = 3 # number of base estimators\n",
    "\n",
    "def generate_fold_ids(train_data, n_folds):\n",
    "    \"\"\"\n",
    "    generate shuffled fold ids for train_data\n",
    "    args:\n",
    "        train_data: numpy array of train data\n",
    "        n_folds: int number of folds\n",
    "    returns:\n",
    "        fold_ids: numpy array of fold ids\n",
    "    \"\"\"\n",
    "    foldsize = N // n_folds\n",
    "    extras = N % n_folds\n",
    "    fold_ids = []\n",
    "    for i in range(n_folds):\n",
    "        fldsz = foldsize\n",
    "        if extras != 0:\n",
    "            fldsz += 1\n",
    "            extras = extras -1\n",
    "        ad = fldsz * [i]\n",
    "        fold_ids += ad\n",
    "    fold_ids = np.array(fold_ids)\n",
    "    np.random.shuffle(fold_ids)\n",
    "    return fold_ids\n",
    "\n",
    "# Partition the training data into five folds\n",
    "fold_ids = generate_fold_ids(train_data, n_folds)\n",
    "\n",
    "# initialize our meta data\n",
    "train_meta = np.zeros((N, n_ests)).astype(int)\n",
    "test_meta = np.zeros((test_data.shape[0], n_ests)).astype(int)\n",
    "\n",
    "# go through each fold and train on all the other data\n",
    "for fold in range(n_folds):\n",
    "    train_ids = []\n",
    "    test_ids = []\n",
    "    for i in range(N):\n",
    "        # get train and test ids for this fold\n",
    "        if fold_ids[i] != fold:\n",
    "            train_ids.append(i)\n",
    "        elif fold_ids[i] == fold:\n",
    "            test_ids.append(i)\n",
    "            \n",
    "    # train on this fold's training data\n",
    "    X = train_data[train_ids]\n",
    "    y = label_train[train_ids]\n",
    "    \n",
    "    print('fitting models in fold', fold)\n",
    "    svc.fit(X, y)\n",
    "    rf.fit(X, y)\n",
    "    knn.fit(X, y)\n",
    "    \n",
    "    # predict on this fold\n",
    "    print('making predictions on fold', fold)\n",
    "    train_meta[test_ids,0] = svc.predict(train_data[test_ids])\n",
    "    train_meta[test_ids,1] = rf.predict(train_data[test_ids])\n",
    "    train_meta[test_ids,2] = knn.predict(train_data[test_ids])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit each base model to the entire training set\n",
    "print('fitting base models to entire training set')\n",
    "svc.fit(train_data, label_train)\n",
    "rf.fit(train_data, label_train)\n",
    "knn.fit(train_data, label_train)\n",
    "\n",
    "# make predictions on test data\n",
    "print('classifying test data with base models')\n",
    "test_meta[:,0] = svc.predict(test_data)\n",
    "test_meta[:,1] = rf.predict(test_data)\n",
    "test_meta[:,2] = knn.predict(test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning the hyperparameters\n",
    "param_grid =  {'penalty': ['l1', 'l2'], 'C': np.logspace(-4, 4, 20), 'solver' : ['liblinear']}\n",
    "reg = LogisticRegression()\n",
    "stack_clf = GridSearchCV(reg, param_grid, cv=5, verbose=True)\n",
    "\n",
    "# Fit on data\n",
    "stack_clf.fit(train_meta, label_train)\n",
    "\n",
    "#print our best meta classifier\n",
    "print(stack_clf.best_estimator_)\n",
    "\n",
    "s_train_pred = stack_clf.predict(train_meta)\n",
    "s_test_pred = stack_clf.predict(test_meta)\n",
    "\n",
    "# print results\n",
    "# train results\n",
    "print('Stacking Results')\n",
    "print('-------------------------')\n",
    "print('Training Data')\n",
    "get_errors(s_train_pred, label_train)\n",
    "\n",
    "# test results\n",
    "print(' ')\n",
    "print('Test Data')\n",
    "s_accuracy,s_recall,s_precision,s_F1 = get_errors(s_test_pred, label_test)\n",
    "\n",
    "accuracies.append(s_accuracy)\n",
    "recalls.append(s_recall)\n",
    "precisions.append(s_precision)\n",
    "F1.append(s_F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# get the roc curve for each model\n",
    "SVM_prob = SV_clf.predict_proba(test_data)[:,1]\n",
    "FPR_SVM, TPR_SVM, _ = roc_curve(label_test, SVM_prob)\n",
    "RF_prob = rf2.predict_proba(test_data)[:,1]\n",
    "FPR_RF, TPR_RF, _ = roc_curve(label_test, RF_prob)\n",
    "KNN_prob = KNN_clf.predict_proba(test_data)[:,1]\n",
    "FPR_KNN, TPR_KNN, _ = roc_curve(label_test, KNN_prob)\n",
    "MV_prob = MV_clf.predict_proba(test_data)[:,1]\n",
    "FPR_MV, TPR_MV, _ = roc_curve(label_test, MV_prob)\n",
    "stack_prob = stack_clf.predict_proba(test_meta)[:,1]\n",
    "FPR_stack, TPR_stack, _ = roc_curve(label_test, stack_prob)\n",
    "\n",
    "\n",
    "# plot the roc curves for each model\n",
    "plt.figure(1, figsize = (10,10))\n",
    "plt.plot(FPR_SVM, TPR_SVM, label='SVM')\n",
    "plt.plot(FPR_RF, TPR_RF, label='RF')\n",
    "plt.plot(FPR_KNN, TPR_KNN, label='KNN')\n",
    "plt.plot(FPR_MV, TPR_MV, label='MV')\n",
    "plt.plot(FPR_stack, TPR_stack, label='stack')\n",
    "\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracies\n",
    "\n",
    "models = ['SVM', 'RF', 'RF2', 'KNN', 'MV', 'Stacking']\n",
    "x = np.arange(len(models))\n",
    "width = 0.30  \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "rects0 = ax.bar(x-width/3, accuracies, width, color='darkorange',label='Accuracy')\n",
    "rects1 = ax.bar(x - width/3, recalls, width, color=\"cadetblue\", label='Recall')\n",
    "rects2 = ax.bar(ind + width/2, precisions, width, color=\"orchid\"\n",
    "                label='Precision')\n",
    "\n",
    "ax.set_title(\"Comparison of Classifiers\", fontsize=20)\n",
    "ax.set_ylabel('Percentage', fontsize=15)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models, fontsize=15)\n",
    "ax.legend()\n",
    "    \n",
    "def autolabel(rects, xpos='center'):\n",
    "    \"\"\"\n",
    "    Attach a text label above each bar in *rects*, displaying its height.\n",
    "\n",
    "    *xpos* indicates which side to place the text w.r.t. the center of\n",
    "    the bar. It can be one of the following {'center', 'right', 'left'}.\n",
    "    \"\"\"\n",
    "\n",
    "    ha = {'center': 'center', 'right': 'left', 'left': 'right'}\n",
    "    offset = {'center': 0, 'right': 1, 'left': -1}\n",
    "\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(offset[xpos]*3, 3),  # use 3 points offset\n",
    "                    textcoords=\"offset points\",  # in both directions\n",
    "                    ha=ha[xpos], va='bottom')\n",
    "\n",
    "\n",
    "autolabel(rects0, \"left\")\n",
    "autolabel(rects1, \"center\")\n",
    "autolabel(rects3, \"right\")\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
